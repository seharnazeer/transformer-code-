{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9858054,"sourceType":"datasetVersion","datasetId":6049711},{"sourceId":9917340,"sourceType":"datasetVersion","datasetId":6094594}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\nimport math\n\nimport warnings\n\nfrom tqdm import tqdm\n\nfrom typing import Any\n\nimport torch\n\nimport torch.nn as nn\n\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\nfrom tqdm import tqdm\n\nfrom pathlib import Path\n\nfrom tokenizers import Tokenizer\n\nfrom tokenizers import Tokenizer, trainers, pre_tokenizers\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\n\nimport os\n\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport transformers\n\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"id":"Ge1CkjEpX5yI","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:55:15.256621Z","iopub.execute_input":"2024-11-15T17:55:15.256928Z","iopub.status.idle":"2024-11-15T17:55:21.197545Z","shell.execute_reply.started":"2024-11-15T17:55:15.256893Z","shell.execute_reply":"2024-11-15T17:55:21.196799Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!conda install -y gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:55:21.198581Z","iopub.execute_input":"2024-11-15T17:55:21.199098Z","iopub.status.idle":"2024-11-15T17:57:02.063849Z","shell.execute_reply.started":"2024-11-15T17:55:21.199063Z","shell.execute_reply":"2024-11-15T17:57:02.062688Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Retrieving notices: ...working... done\nChannels:\n - rapidsai\n - nvidia\n - nodefaults\n - conda-forge\n - defaults\n - pytorch\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - gdown\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    conda-24.9.2               |  py310hff52083_0         895 KB  conda-forge\n    filelock-3.16.1            |     pyhd8ed1ab_0          17 KB  conda-forge\n    gdown-5.2.0                |     pyhd8ed1ab_0          21 KB  conda-forge\n    openssl-3.4.0              |       hb9d3cd8_0         2.8 MB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         3.7 MB\n\nThe following NEW packages will be INSTALLED:\n\n  filelock           conda-forge/noarch::filelock-3.16.1-pyhd8ed1ab_0 \n  gdown              conda-forge/noarch::gdown-5.2.0-pyhd8ed1ab_0 \n\nThe following packages will be UPDATED:\n\n  conda                              24.9.0-py310hff52083_0 --> 24.9.2-py310hff52083_0 \n  openssl                                  3.3.2-hb9d3cd8_0 --> 3.4.0-hb9d3cd8_0 \n\n\n\nDownloading and Extracting Packages:\nopenssl-3.4.0        | 2.8 MB    |                                       |   0% \nconda-24.9.2         | 895 KB    |                                       |   0% \u001b[A\n\ngdown-5.2.0          | 21 KB     |                                       |   0% \u001b[A\u001b[A\n\n\nopenssl-3.4.0        | 2.8 MB    | ##########                            |  27% \u001b[A\u001b[A\u001b[A\n\n\nfilelock-3.16.1      | 17 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\ngdown-5.2.0          | 21 KB     | ##################################### | 100% \u001b[A\u001b[A\n\n\nfilelock-3.16.1      | 17 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\nopenssl-3.4.0        | 2.8 MB    | ##################################### | 100% \u001b[A\u001b[A\nconda-24.9.2         | 895 KB    | ##################################### | 100% \u001b[A\n                                                                                \u001b[A\n                                                                                \u001b[A\n\n                                                                                \u001b[A\u001b[A\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:02.065353Z","iopub.execute_input":"2024-11-15T17:57:02.065684Z","iopub.status.idle":"2024-11-15T17:57:13.831363Z","shell.execute_reply.started":"2024-11-15T17:57:02.065642Z","shell.execute_reply":"2024-11-15T17:57:13.830290Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Enable parallelism in tokenization\n# transformers.utils.logging.enable_propagation()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.832781Z","iopub.execute_input":"2024-11-15T17:57:13.833145Z","iopub.status.idle":"2024-11-15T17:57:13.837423Z","shell.execute_reply.started":"2024-11-15T17:57:13.833109Z","shell.execute_reply":"2024-11-15T17:57:13.836502Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"**Embedding**","metadata":{"id":"s9EjH3LtYJXR"}},{"cell_type":"code","source":"class Embedding(nn.Module):\n\n  def __init__(self, d_model:int, vocab_size:int):\n\n    super().__init__()\n\n    # model dimension from the paper whihc is 512\n\n    self.d_model=d_model\n\n    self.vocab_size=vocab_size\n\n    self.embedding=nn.Embedding(vocab_size,d_model)\n\n  def forward(self,x):\n\n    # * sqrt(self.d_model) from the research paper\n\n    return self.embedding(x) * math.sqrt(self.d_model)","metadata":{"id":"vHaeRuqhX9I_","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.840992Z","iopub.execute_input":"2024-11-15T17:57:13.841303Z","iopub.status.idle":"2024-11-15T17:57:13.849063Z","shell.execute_reply.started":"2024-11-15T17:57:13.841252Z","shell.execute_reply":"2024-11-15T17:57:13.848122Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n  def __init__(self, d_model:int , seq_len:int , dropout:float):\n\n    super().__init__()\n\n    self.d_model=d_model\n\n    self.seq_len=seq_len\n\n    self.dropout=nn.Dropout(dropout)\n\n\n\n    pe=torch.zeros(self.seq_len, self.d_model)\n\n    # unsequeeze 1 to reshape\n\n    positions=torch.arange(0, self.seq_len, dtype=torch.float).unsqueeze(1)\n\n    div_term = 10000 ** (torch.arange(0,self.d_model,2) / d_model)\n\n    # even poistion encoding\n\n    pe[:,0::2]=torch.sin(positions/div_term)\n\n    # odd poistion encoding\n\n    pe[:,1::2]=torch.cos(positions/div_term)\n\n    # for bacth dimensions\n\n    pe=pe.unsqueeze(0)\n\n    # saving our positional encoding like tunable parameter but it did not update during training\n\n    self.register_buffer(\"pe\",pe)\n\n  def forward(self,x):\n\n    # not want trainable encoding\n\n    x=x+ (self.pe[:,:x.shape[1],:]).requires_grad_(False)\n\n    return self.dropout(x)","metadata":{"id":"y0ZRNomBbWlh","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.850220Z","iopub.execute_input":"2024-11-15T17:57:13.850625Z","iopub.status.idle":"2024-11-15T17:57:13.859886Z","shell.execute_reply.started":"2024-11-15T17:57:13.850583Z","shell.execute_reply":"2024-11-15T17:57:13.859007Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# **MultiHead Attention**","metadata":{"id":"6SDLFg1JrZ63"}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n\n  def __init__(self, d_model:int,h:int, dropout:float ):\n\n    super().__init__()\n\n    self.d_model=d_model\n\n    self.h=h\n\n\n\n    assert d_model % h==0,\"Dimensions is not divisible by number of heads\"\n\n\n\n    self.d_k=self.d_model // self.h\n\n    #  now query key and value weights\n\n    self.w_q=nn.Linear(d_model, d_model)\n\n    self.w_k=nn.Linear(d_model, d_model)\n\n    self.w_v=nn.Linear(d_model, d_model)\n\n    # matrix which we use after concatenating to convert it to same dimensional back\n\n    self.w_o=nn.Linear(d_model, d_model)\n\n    self.dropout=nn.Dropout(dropout)\n\n\n\n  @staticmethod\n\n  def attention(query, key, value, mask, dropout: nn.Dropout):\n\n    # mask => When we want certain words to NOT interact with others, we \"hide\" them\n\n\n\n    d_k = query.shape[-1] # The last dimension of query, key, and value\n\n\n\n    # We calculate the Attention(Q,K,V) as in the formula in the image above\n\n    attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k) # @ = Matrix multiplication sign in PyTorch\n\n\n    if mask is not None: \n\n        attention_scores.masked_fill_(mask == 0, -1e9) # Replace each value where mask is equal to 0 by -1e9\n\n        attention_scores = attention_scores.softmax(dim = -1) # Applying softmax\n\n        if dropout is not None: # If a dropout IS defined...\n\n            attention_scores = dropout(attention_scores) # We apply dropout to prevent overfitting\n\n\n\n    return (attention_scores @ value), attention_scores # Multiply the output matrix by the V matrix, as in the formula\n\n\n  def forward(self, q, k, v, mask):\n\n    query=self.w_q(q)\n\n    key=self.w_k(k)\n\n    value=self.w_v(v)\n\n    # reshaping it for multihead\n\n    query=query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n\n    key=key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n\n    value=value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n\n    # Obtaining the output and the attention scores\n\n    x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n\n\n\n    # Obtaining the H matrix\n\n    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n\n\n\n    return self.w_o(x) # Multiply the H matrix by the weight matrix W_o, resulting in the MH-A matrix","metadata":{"id":"iro6I3CdbWo7","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.861457Z","iopub.execute_input":"2024-11-15T17:57:13.861817Z","iopub.status.idle":"2024-11-15T17:57:13.875708Z","shell.execute_reply.started":"2024-11-15T17:57:13.861774Z","shell.execute_reply":"2024-11-15T17:57:13.874880Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"**Residual Connections**","metadata":{"id":"9GhKsldVciqm"}},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n\n    def __init__(self, dropout: float) -> None:\n\n        super().__init__()\n\n        self.dropout = nn.Dropout(dropout) # We use a dropout layer to prevent overfitting\n\n        self.norm = LayerNormalization() # We use a normalization layer\n\n\n\n    def forward(self, x, sublayer):\n\n        # We normalize the input and add it to the original input 'x'. This creates the residual connection process.\n\n        return x + self.dropout(self.norm(sublayer(x)))","metadata":{"id":"7HJuYxYvbWsB","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.876840Z","iopub.execute_input":"2024-11-15T17:57:13.877199Z","iopub.status.idle":"2024-11-15T17:57:13.888992Z","shell.execute_reply.started":"2024-11-15T17:57:13.877159Z","shell.execute_reply":"2024-11-15T17:57:13.888185Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Feed Forward Network**","metadata":{"id":"EtPTepVzc2Ca"}},{"cell_type":"markdown","source":"dff=2048\n\nfrom the paper attention all you need","metadata":{"id":"5sQhXub4c8lj"}},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n\n\n\n    def __init__(self, eps: float = 10**-6) -> None: # We define epsilon as 0.000001 to avoid division by zero\n\n        super().__init__()\n\n        self.eps = eps\n\n        self.alpha = nn.Parameter(torch.ones(1)) \n\n\n\n        # We define bias as a trainable parameter and initialize it with zeros\n\n        self.bias = nn.Parameter(torch.zeros(1)) \n\n\n\n    def forward(self, x):\n\n        mean = x.mean(dim = -1, keepdim = True)\n\n        std = x.std(dim = -1, keepdim = True) \n\n\n\n        # Returning the normalized input\n\n        return self.alpha * (x-mean) / (std + self.eps) + self.bias","metadata":{"id":"a0hRXKjcxo1G","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.890122Z","iopub.execute_input":"2024-11-15T17:57:13.890708Z","iopub.status.idle":"2024-11-15T17:57:13.898451Z","shell.execute_reply.started":"2024-11-15T17:57:13.890667Z","shell.execute_reply":"2024-11-15T17:57:13.897655Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\n\nclass FeedForwardBlock(nn.Module):\n\n\n\n    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n\n        super().__init__()\n\n        # First linear transformation\n\n        self.linear_1 = nn.Linear(d_model, d_ff) # W1 & b1\n\n        self.dropout = nn.Dropout(dropout) # Dropout\n\n        # Second linear transformation\n\n        self.linear_2 = nn.Linear(d_ff, d_model) # W2 & b2\n\n\n\n    def forward(self, x):\n\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))","metadata":{"id":"DmWJbtdfbWvK","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.899478Z","iopub.execute_input":"2024-11-15T17:57:13.899797Z","iopub.status.idle":"2024-11-15T17:57:13.908926Z","shell.execute_reply.started":"2024-11-15T17:57:13.899758Z","shell.execute_reply":"2024-11-15T17:57:13.908129Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"**Encoder Block**","metadata":{"id":"d0XbZiz0tyEo"}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n\n\n\n    def __init__(self, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n\n        super().__init__()\n\n        # Storing the self-attention block and feed-forward block\n\n        self.self_attention_block = self_attention_block\n\n        self.feed_forward_block = feed_forward_block\n\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # 2 Residual Connections with dropout\n\n\n\n    def forward(self, x, src_mask):\n\n        # applying the first residual connection with the self-attention block\n\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n\n        x = self.residual_connections[1](x, self.feed_forward_block)\n\n        return x","metadata":{"id":"q4rS16YJbWyA","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.910073Z","iopub.execute_input":"2024-11-15T17:57:13.910360Z","iopub.status.idle":"2024-11-15T17:57:13.918296Z","shell.execute_reply.started":"2024-11-15T17:57:13.910329Z","shell.execute_reply":"2024-11-15T17:57:13.917439Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"**Complete Encoder**","metadata":{"id":"grN3hgOaEN9j"}},{"cell_type":"code","source":"class Encoder(nn.Module):\n\n\n\n    # The Encoder takes in instances of 'EncoderBlock'\n\n    def __init__(self, layers: nn.ModuleList) -> None:\n\n        super().__init__()\n\n        self.layers = layers # Storing the EncoderBlocks\n\n        self.norm = LayerNormalization() # Layer for the normalization of the output of the encoder layers\n\n\n\n    def forward(self, x, mask):\n\n        # Iterating over each EncoderBlock stored in self.layers\n\n        for layer in self.layers:\n\n            x = layer(x, mask) # Applying each EncoderBlock to the input tensor\n\n        return self.norm(x) # Normalizing output","metadata":{"id":"Kvm3TNeRbW1L","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.919440Z","iopub.execute_input":"2024-11-15T17:57:13.919802Z","iopub.status.idle":"2024-11-15T17:57:13.927038Z","shell.execute_reply.started":"2024-11-15T17:57:13.919761Z","shell.execute_reply":"2024-11-15T17:57:13.926251Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n\n\n\n    # The DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention, while the other is cross-attention.\n\n    def __init__(self,  self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n\n        super().__init__()\n\n        self.self_attention_block = self_attention_block\n\n        self.cross_attention_block = cross_attention_block\n\n        self.feed_forward_block = feed_forward_block\n\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)]) # List of three Residual Connections with dropout rate\n\n\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n\n\n\n        # Self-Attention block with query, key, and value plus the target language mask\n\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n\n        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n\n\n\n        # Feed-forward block with residual connections\n\n        x = self.residual_connections[2](x, self.feed_forward_block)\n\n        return x","metadata":{"id":"GbLC7UuDbW4A","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.928299Z","iopub.execute_input":"2024-11-15T17:57:13.928879Z","iopub.status.idle":"2024-11-15T17:57:13.938620Z","shell.execute_reply.started":"2024-11-15T17:57:13.928838Z","shell.execute_reply":"2024-11-15T17:57:13.937839Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\n\n\n\nclass Decoder(nn.Module):\n\n\n\n    # The Decoder takes in instances of 'DecoderBlock'\n\n    def __init__(self, layers: nn.ModuleList) -> None:\n\n        super().__init__()\n\n        self.layers = layers\n\n        self.norm = LayerNormalization() # Layer to normalize the output\n\n\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n\n\n\n        # iterating over each decoderBlock stored in self.layers\n\n        for layer in self.layers:\n\n            # applies each DecoderBlock to the input 'x' plus the encoder output and source and target masks\n\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n\n        return self.norm(x) # Returns normalized output","metadata":{"id":"vuvGHK9_bW7I","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.939827Z","iopub.execute_input":"2024-11-15T17:57:13.940726Z","iopub.status.idle":"2024-11-15T17:57:13.947737Z","shell.execute_reply.started":"2024-11-15T17:57:13.940670Z","shell.execute_reply":"2024-11-15T17:57:13.946907Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"**Linear Layer**","metadata":{"id":"mc8fDvXpOs4b"}},{"cell_type":"code","source":"class ProjectionLayer(nn.Module):\n\n    def __init__(self, d_model: int, vocab_size: int) -> None: # Model dimension and the size of the output vocabulary\n\n        super().__init__()\n\n        self.proj = nn.Linear(d_model, vocab_size) # Linear layer for projecting the feature space of 'd_model' to the output space of 'vocab_size'\n\n    def forward(self, x):\n\n        return torch.log_softmax(self.proj(x), dim = -1)","metadata":{"id":"kt0uoBlobW9_","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.948761Z","iopub.execute_input":"2024-11-15T17:57:13.949280Z","iopub.status.idle":"2024-11-15T17:57:13.958696Z","shell.execute_reply.started":"2024-11-15T17:57:13.949248Z","shell.execute_reply":"2024-11-15T17:57:13.957717Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# **Transformer **","metadata":{"id":"MxN_WkkERCj8"}},{"cell_type":"code","source":"class Transformer(nn.Module):\n\n    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: Embedding, tgt_embed: Embedding, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n\n        super().__init__()\n\n        self.encoder = encoder\n\n        self.decoder = decoder\n\n        self.src_embed = src_embed\n\n        self.tgt_embed = tgt_embed\n\n        self.src_pos = src_pos\n\n        self.tgt_pos = tgt_pos\n\n        self.projection_layer = projection_layer\n\n\n\n    # Encoder\n\n    def encode(self, src, src_mask):\n\n        src = self.src_embed(src) # Applying source embeddings to the input source language\n\n        src = self.src_pos(src) # Applying source positional encoding to the source embeddings\n\n        return self.encoder(src, src_mask) # Returning the source embeddings plus a source mask to prevent attention to certain elements\n\n\n\n    # Decoder\n\n    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n\n        tgt = self.tgt_embed(tgt) # Applying target embeddings to the input target language (tgt)\n\n        tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n\n        # The target mask ensures that the model won't 'see' future elements of the sequence\n\n        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n\n\n\n    # Applying Projection Layer with the Softmax function to the Decoder output\n\n    def project(self, x):\n\n        return self.projection_layer(x)","metadata":{"id":"Gvk3CBbVbXA7","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.959732Z","iopub.execute_input":"2024-11-15T17:57:13.960046Z","iopub.status.idle":"2024-11-15T17:57:13.969178Z","shell.execute_reply.started":"2024-11-15T17:57:13.960014Z","shell.execute_reply":"2024-11-15T17:57:13.968076Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n\n\n\n    # Creating Embedding layers\n\n    src_embed = Embedding(d_model, src_vocab_size) # Source language (Source Vocabulary to 512-dimensional vectors)\n\n    tgt_embed = Embedding(d_model, tgt_vocab_size) # Target language (Target Vocabulary to 512-dimensional vectors)\n\n\n\n    # Creating Positional Encoding layers\n\n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # Positional encoding for the source language embeddings\n\n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # Positional encoding for the target language embeddings\n\n\n\n    # Creating EncoderBlocks\n\n    encoder_blocks = [] # Initial list of empty EncoderBlocks\n\n    for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n\n        encoder_self_attention_block = MultiHeadAttention(d_model, h, dropout) # Self-Attention\n\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n\n\n\n        # Combine layers into an EncoderBlock\n\n        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n\n        encoder_blocks.append(encoder_block) # Appending EncoderBlock to the list of EncoderBlocks\n\n\n\n    # Creating DecoderBlocks\n\n    decoder_blocks = [] # Initial list of empty DecoderBlocks\n\n    for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n\n        decoder_self_attention_block = MultiHeadAttention(d_model, h, dropout) # Self-Attention\n\n        decoder_cross_attention_block = MultiHeadAttention(d_model, h, dropout) # Cross-Attention\n\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n\n\n\n        # Combining layers into a DecoderBlock\n\n        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n\n        decoder_blocks.append(decoder_block) # Appending DecoderBlock to the list of DecoderBlocks\n\n\n\n    # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n\n    encoder = Encoder(nn.ModuleList(encoder_blocks))\n\n    decoder = Decoder(nn.ModuleList(decoder_blocks))\n\n\n\n    # Creating projection layer\n\n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # Map the output of Decoder to the Target Vocabulary Space\n\n\n\n    # Creating the transformer by combining everything above\n\n    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n\n\n\n    # Initialize the parameters\n\n    for p in transformer.parameters():\n\n        if p.dim() > 1:\n\n            nn.init.xavier_uniform_(p)\n\n\n\n    return transformer # Assembled and initialized Transformer. Ready to be trained and validated!\n","metadata":{"id":"rYWs_Xj3bXEK","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.970389Z","iopub.execute_input":"2024-11-15T17:57:13.970667Z","iopub.status.idle":"2024-11-15T17:57:13.982468Z","shell.execute_reply.started":"2024-11-15T17:57:13.970632Z","shell.execute_reply":"2024-11-15T17:57:13.981611Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\n\ndef build_tokenizer(config, ds, lang):\n    # Creating a file path for the tokenizer\n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n\n    # Checking if Tokenizer already exists\n    if not Path.exists(tokenizer_path):\n        # If it doesn't exist, we create a new one with BPE\n        tokenizer = Tokenizer(BPE(unk_token='[UNK]'))  # Initializing a new BPE tokenizer\n        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()  # We will split text based on whitespace\n\n        # Creating a BPE trainer with special tokens and minimum frequency\n        trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2, vocab_size=30000)  # Adjust vocab size as needed\n\n        # Train the tokenizer on sentences from the dataset and specified language\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n        tokenizer.save(str(tokenizer_path))  # Save the trained tokenizer\n    else:\n        # If the tokenizer already exists, load it\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n\n    return tokenizer  # Return the loaded or newly trained tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.983558Z","iopub.execute_input":"2024-11-15T17:57:13.983900Z","iopub.status.idle":"2024-11-15T17:57:13.993562Z","shell.execute_reply.started":"2024-11-15T17:57:13.983856Z","shell.execute_reply":"2024-11-15T17:57:13.992843Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def casual_mask(size):\n        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n        return mask == 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:13.994699Z","iopub.execute_input":"2024-11-15T17:57:13.995014Z","iopub.status.idle":"2024-11-15T17:57:14.003065Z","shell.execute_reply.started":"2024-11-15T17:57:13.994979Z","shell.execute_reply":"2024-11-15T17:57:14.002308Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def get_all_sentences(ds, lang):\n    for pair in ds:\n        yield pair['translation'][lang]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:14.009412Z","iopub.execute_input":"2024-11-15T17:57:14.009723Z","iopub.status.idle":"2024-11-15T17:57:14.014394Z","shell.execute_reply.started":"2024-11-15T17:57:14.009683Z","shell.execute_reply":"2024-11-15T17:57:14.013425Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from datasets import load_dataset\nfrom torch.utils.data import DataLoader, random_split\n\ndef get_ds(config):\n    # Load the WMT2014 English-German dataset from Hugging Face Datasets\n    from datasets import load_from_disk\n\n    # Load the saved subset\n    ds_raw = load_from_disk(\"wmt14_en_de_subset_100k\")\n\n    # Verify the loaded subset\n    print(f\"Loaded subset size: {len(ds_raw)}\")\n\n    # Initialize the tokenizers for source (English) and target (German) languages\n    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n\n    # Split the dataset for training and validation\n    train_ds_size = int(0.9 * len(ds_raw))  # 90% for training\n    val_ds_size = len(ds_raw) - train_ds_size  # 10% for validation\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n\n    # Process data with the BilingualDataset class\n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n\n    # Calculate the maximum sentence length in source and target sentences for reference\n    max_len_src = 0\n    max_len_tgt = 0\n    for pair in ds_raw:\n        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_tgt.encode(pair['translation'][config['lang_tgt']]).ids\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n    print(f'Max length of source sentence: {max_len_src}')\n    print(f'Max length of target sentence: {max_len_tgt}')\n\n    # Create dataloaders for the training and validation sets\n    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n\n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:14.015449Z","iopub.execute_input":"2024-11-15T17:57:14.015748Z","iopub.status.idle":"2024-11-15T17:57:14.245668Z","shell.execute_reply.started":"2024-11-15T17:57:14.015718Z","shell.execute_reply":"2024-11-15T17:57:14.244892Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class BilingualDataset(Dataset):\n\n    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages\n    # 'seq_len' defines the sequence length for both languages\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n        super().__init__()\n\n        self.seq_len = seq_len\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n\n        # Defining special tokens by using the target language tokenizer\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n\n    # Total number of instances in the dataset (some pairs are larger than others)\n    def __len__(self):\n        return len(self.ds)\n\n    # Using the index to retrive source and target texts\n    def __getitem__(self, index: Any) -> Any:\n        src_target_pair = self.ds[index]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]\n\n        # Tokenizing source and target texts\n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n\n        # Computing how many padding tokens need to be added to the tokenized texts\n        # Source tokens\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n        # Target tokens\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n\n        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n        # given the current sequence length limit (this will be defined in the config dictionary below)\n        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n            raise ValueError('Sentence is too long')\n\n        # Building the encoder input tensor by combining several elements\n        encoder_input = torch.cat(\n            [\n            self.sos_token, # inserting the '[SOS]' token\n            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n            self.eos_token, # Inserting the '[EOS]' token\n            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n            ]\n        )\n\n        # Building the decoder input tensor by combining several elements\n        decoder_input = torch.cat(\n            [\n                self.sos_token, # inserting the '[SOS]' token\n                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n            ]\n\n        )\n\n        # Creating a label tensor, the expected output for training the model\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n                self.eos_token, # Inserting the '[EOS]' token\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n\n            ]\n        )\n\n        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n\n        return {\n            'encoder_input': encoder_input,\n            'decoder_input': decoder_input,\n            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n            'label': label,\n            'src_text': src_text,\n            'tgt_text': tgt_text\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:14.246868Z","iopub.execute_input":"2024-11-15T17:57:14.247772Z","iopub.status.idle":"2024-11-15T17:57:14.264076Z","shell.execute_reply.started":"2024-11-15T17:57:14.247733Z","shell.execute_reply":"2024-11-15T17:57:14.263195Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    # Retrieving the indices from the start and end of sequences of the target tokens\n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n\n    # Computing the output of the encoder for the source sequence\n    encoder_output = model.encode(source, source_mask)\n    # Initializing the decoder input with the Start of Sentence token\n    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n\n    # Looping until the 'max_len', maximum length, is reached\n    while True:\n        if decoder_input.size(1) == max_len:\n            break\n\n        # Building a mask for the decoder input\n        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n\n        # Calculating the output of the decoder\n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n\n        # Applying the projection layer to get the probabilities for the next token\n        prob = model.project(out[:, -1])\n\n        # Selecting token with the highest probability\n        _, next_word = torch.max(prob, dim=1)\n        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n\n        # If the next token is an End of Sentence token, we finish the loop\n        if next_word == eos_idx:\n            break\n\n    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:14.265312Z","iopub.execute_input":"2024-11-15T17:57:14.265644Z","iopub.status.idle":"2024-11-15T17:57:14.278565Z","shell.execute_reply.started":"2024-11-15T17:57:14.265610Z","shell.execute_reply":"2024-11-15T17:57:14.277727Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n    model.eval() # Setting model to evaluation mode\n    count = 0 # Initializing counter to keep track of how many examples have been processed\n\n    console_width = 80 # Fixed witdh for printed messages\n\n    # Creating evaluation loop\n    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch['encoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n\n            # Ensuring that the batch_size of the validation set is 1\n            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n\n            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n            # Retrieving source and target texts from the batch\n            source_text = batch['src_text'][0]\n            target_text = batch['tgt_text'][0] # True translation\n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n\n            # Printing results\n            print_msg('-'*console_width)\n            print_msg(f'SOURCE: {source_text}')\n            print_msg(f'TARGET: {target_text}')\n            print_msg(f'PREDICTED: {model_out_text}')\n\n            # After two examples, we break the loop\n            if count == num_examples:\n                break\n\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:14.279685Z","iopub.execute_input":"2024-11-15T17:57:14.280066Z","iopub.status.idle":"2024-11-15T17:57:14.290895Z","shell.execute_reply.started":"2024-11-15T17:57:14.280033Z","shell.execute_reply":"2024-11-15T17:57:14.290120Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n    return model\n\n  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:14.291901Z","iopub.execute_input":"2024-11-15T17:57:14.292496Z","iopub.status.idle":"2024-11-15T17:57:14.302324Z","shell.execute_reply.started":"2024-11-15T17:57:14.292451Z","shell.execute_reply":"2024-11-15T17:57:14.301481Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def get_config():\n    return{\n        'batch_size': 8,\n        'num_epochs': 10,\n        'lr': 10**-4,\n        'seq_len': 350,\n        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n        'lang_src': 'en',\n        'lang_tgt': 'de',\n        'model_folder': 'weights',\n        'model_basename': 'tmodel_000',\n        'preload': 8,\n        'tokenizer_file': 'tokenizer_{0}.json',\n        'experiment_name': 'runs/tmodel'\n    }\n\n\n# Function to construct the path for saving and retrieving model weights\ndef get_weights_file_path(config, epoch: str):\n    model_folder = config['model_folder'] # Extracting model folder from the config\n    model_basename = config['model_basename'] # Extracting the base name for model files\n    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n    return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename\n\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:14.303644Z","iopub.execute_input":"2024-11-15T17:57:14.304188Z","iopub.status.idle":"2024-11-15T17:57:14.310903Z","shell.execute_reply.started":"2024-11-15T17:57:14.304155Z","shell.execute_reply":"2024-11-15T17:57:14.310148Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def train_model(config):\n    # Setting up device to run on GPU to train faster\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device {device}\")\n\n    # Creating model directory to store weights\n    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n\n    # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n\n    # Initializing model on the GPU using the 'get_model' function\n    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n\n    # Tensorboard\n    writer = SummaryWriter(config['experiment_name'])\n\n    # Setting up the Adam optimizer with the specified learning rate from the '\n    # config' dictionary plus an epsilon value\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n\n    # Initializing epoch and global step variables\n    initial_epoch = 2\n    global_step = 0\n\n    # Checking if there is a pre-trained model to load\n    # If true, loads it\n    if config['preload']:\n        model_filename = get_weights_file_path(config, config['preload'])\n        print(f'Preloading model {model_filename}')\n        state = torch.load(model_filename) # Loading model\n        model.load_state_dict(state['model_state_dict'])\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n        print(f\"Resumed from epoch {initial_epoch}, global step {global_step}\")\n\n        # Sets epoch to the saved in the state plus one, to resume from where it stopped\n        initial_epoch = state['epoch'] + 1\n\n    # We also apply label_smoothing to prevent overfitting\n    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n\n    # Initializing training loop\n\n    # Iterating over each epoch from the 'initial_epoch' variable up to\n    # the number of epochs informed in the config\n    for epoch in range(initial_epoch, config['num_epochs']):\n\n        # Initializing an iterator over the training dataloader\n        # We also use tqdm to display a progress bar\n        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n\n        # For each batch...\n        for batch in batch_iterator:\n            model.train() # Train the model\n\n            # Loading input data and masks onto the GPU\n            encoder_input = batch['encoder_input'].to(device)\n            decoder_input = batch['decoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n            decoder_mask = batch['decoder_mask'].to(device)\n\n            # Running tensors through the Transformer\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n            proj_output = model.project(decoder_output)\n\n            # Loading the target labels onto the GPU\n            label = batch['label'].to(device)\n\n            # Computing loss between model's output and true labels\n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n\n            # Updating progress bar\n            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n\n            writer.add_scalar('train loss', loss.item(), global_step)\n            writer.flush()\n\n            # Performing backpropagation\n            loss.backward()\n\n            # Updating parameters based on the gradients\n            optimizer.step()\n\n            # Clearing the gradients to prepare for the next batch\n            optimizer.zero_grad()\n\n            global_step += 1 # Updating global step count\n\n        # We run the 'run_validation' function at the end of each epoch\n        # to evaluate model performance\n        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n\n        # Saving model\n        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n        # Writting current model state to the 'model_filename'\n        torch.save({\n            'epoch': epoch, # Current epoch\n            'model_state_dict': model.state_dict(),# Current model state\n            'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n            'global_step': global_step # Current global step\n        }, model_filename)\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:14.312070Z","iopub.execute_input":"2024-11-15T17:57:14.312350Z","iopub.status.idle":"2024-11-15T17:57:14.329780Z","shell.execute_reply.started":"2024-11-15T17:57:14.312319Z","shell.execute_reply":"2024-11-15T17:57:14.328949Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# below code is for testing the model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:14.330997Z","iopub.execute_input":"2024-11-15T17:57:14.331601Z","iopub.status.idle":"2024-11-15T17:57:14.340277Z","shell.execute_reply.started":"2024-11-15T17:57:14.331559Z","shell.execute_reply":"2024-11-15T17:57:14.339354Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def greedy(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    # Retrieve the start and end of sequence tokens\n    sos_idx = tokenizer_tgt.convert_tokens_to_ids('[SOS]')  # Start of Sentence\n    eos_idx = tokenizer_tgt.convert_tokens_to_ids('[EOS]')  # End of Sentence\n\n    # Initialize decoder input with the SOS token\n    decoder_input = torch.tensor([[sos_idx]], device=device)\n\n    # Prepare to store the generated tokens\n    generated_tokens = []\n\n    with torch.no_grad():\n        # Pass the source through the encoder\n        encoder_output = model.encode(source, source_mask)\n\n        for _ in range(max_len):\n            # Create a decoder mask\n            decoder_mask = (decoder_input != tokenizer_src.pad_token_id).unsqueeze(1).to(device)\n\n            # Pass encoder output and decoder input through the model to get output\n            decoder_output = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n            proj_output = model.project(decoder_output)\n\n            # Get the next token (greedy selection)\n            next_token = proj_output.argmax(-1)[:, -1].item()\n            generated_tokens.append(next_token)\n\n            # Stop if EOS token is generated\n            if next_token == eos_idx:\n                break\n\n            # Add the predicted token to decoder input for the next iteration\n            decoder_input = torch.cat([decoder_input, torch.tensor([[next_token]], device=device)], dim=1)\n\n    return torch.tensor(generated_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:14.341417Z","iopub.execute_input":"2024-11-15T17:57:14.341724Z","iopub.status.idle":"2024-11-15T17:57:14.351308Z","shell.execute_reply.started":"2024-11-15T17:57:14.341672Z","shell.execute_reply":"2024-11-15T17:57:14.350134Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"config=get_config()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# 2. Load tokenizers and data\ntrain_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n\n# 3. Initialize the model and load weights if available\nmodel = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n\n# Check if there are pre-trained weights to load\nif config.get('preload'):\n    model_filename = get_weights_file_path(config, config['preload'])\n    print(f'Loading pre-trained model from {model_filename}')\n    state = torch.load(model_filename)\n    model.load_state_dict(state['model_state_dict'])\n\n# 4. Set the maximum sequence length for translation\nmax_len = config['seq_len']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:14.352342Z","iopub.execute_input":"2024-11-15T17:57:14.352606Z","iopub.status.idle":"2024-11-15T17:57:31.364814Z","shell.execute_reply.started":"2024-11-15T17:57:14.352576Z","shell.execute_reply":"2024-11-15T17:57:31.363987Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoaded subset size: 100000\nMax length of source sentence: 300\nMax length of target sentence: 266\nLoading pre-trained model from weights/tmodel_0008.pt\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_105/1111915620.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(model_filename)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\n# Load tokenizers\ntokenizer_src = PreTrainedTokenizerFast(tokenizer_file=\"/kaggle/working/tokenizer_en.json\")\ntokenizer_tgt = PreTrainedTokenizerFast(tokenizer_file=\"/kaggle/working/tokenizer_de.json\")\n\n# Set special tokens if they're not automatically set\nif tokenizer_src.pad_token is None:\n    tokenizer_src.pad_token = \"[PAD]\"\nif tokenizer_tgt.pad_token is None:\n    tokenizer_tgt.pad_token = \"[PAD]\"\n\n# Define the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load model architecture and weights\ndef load_model(config):\n    model = get_model(config, tokenizer_src.vocab_size, tokenizer_tgt.vocab_size).to(device)\n    model_path = \"/kaggle/working/weights/tmodel_0007.pt\"\n    checkpoint = torch.load(model_path,weights_only=True)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    return model\n\n# Translation inference function\ndef translate_sentence_inference(model, sentence, tokenizer_src, tokenizer_tgt, max_len, device):\n    model.eval()  # Set model to evaluation mode\n\n    # Tokenize the input sentence and convert it to a tensor\n    input_tokens = tokenizer_src.encode(sentence, add_special_tokens=True)  # Returns list of token IDs\n    input_tensor = torch.tensor(input_tokens).unsqueeze(0).to(device)  # Add batch dimension\n\n    # Set pad_token_id\n    pad_token_id = tokenizer_src.pad_token_id if tokenizer_src.pad_token_id is not None else 0\n    encoder_mask = (input_tensor != pad_token_id).unsqueeze(1).to(device)  # Create mask\n\n    # Generate translation using the greedy decoding method\n    model_out = greedy(model, input_tensor, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n    # Decode the output tokens to get the translated sentence\n    translated_sentence = tokenizer_tgt.decode(model_out.cpu().numpy(), skip_special_tokens=True)\n\n    return translated_sentence\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:31.366212Z","iopub.execute_input":"2024-11-15T17:57:31.366600Z","iopub.status.idle":"2024-11-15T17:57:31.426044Z","shell.execute_reply.started":"2024-11-15T17:57:31.366553Z","shell.execute_reply":"2024-11-15T17:57:31.425063Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# --- SPEECH-TO-TEXT ---\nimport whisper\ndef convert_audio_to_text(file_path):\n    \"\"\"Converts an audio file to text using the Whisper model.\"\"\"\n    model = whisper.load_model(\"base\")  # Load a pre-trained Whisper model\n    print(\"Transcribing audio to text...\")\n    result = model.transcribe(file_path)\n    transcript = result['text']\n    print(\"Transcription:\", transcript)\n    return transcript","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:31.427728Z","iopub.execute_input":"2024-11-15T17:57:31.428482Z","iopub.status.idle":"2024-11-15T17:57:32.368509Z","shell.execute_reply.started":"2024-11-15T17:57:31.428436Z","shell.execute_reply":"2024-11-15T17:57:32.367739Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from gtts import gTTS\n\n# Function to convert text to speech and save as file\ndef text_to_speech(text, lang='de'):\n    # Convert text to speech using Google Text-to-Speech\n    tts = gTTS(text=text, lang=lang, slow=False)  # slow=False makes the speech faster\n    # Save the speech to a file\n    filename = \"output.mp3\"\n    tts.save(filename)\n    print(f\"Audio file saved as {filename}. You can download it from the output files section.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:32.369656Z","iopub.execute_input":"2024-11-15T17:57:32.370066Z","iopub.status.idle":"2024-11-15T17:57:32.381607Z","shell.execute_reply.started":"2024-11-15T17:57:32.370021Z","shell.execute_reply":"2024-11-15T17:57:32.380742Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"text=convert_audio_to_text(\"/kaggle/input/fortest/ElevenLabs_Text_to_Speech_audio.mp3\")\n\nmodel = load_model(config)\n\n\n# Translate and evaluate\ntranslated_sentence = translate_sentence_inference(model,text, tokenizer_src, tokenizer_tgt, max_len=350, device=device)\nprint(\"actual sentence\", text)\nprint(\"Translated sentence:\", translated_sentence)\ntext_to_speech(translated_sentence)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:58:02.517368Z","iopub.execute_input":"2024-11-15T17:58:02.517773Z","iopub.status.idle":"2024-11-15T17:58:07.035303Z","shell.execute_reply.started":"2024-11-15T17:58:02.517733Z","shell.execute_reply":"2024-11-15T17:58:07.034249Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Transcribing audio to text...\nTranscription:  The European Union is a unique partnership between 27 European countries, known as member states or EU countries.\nactual sentence  The European Union is a unique partnership between 27 European countries, known as member states or EU countries.\nTranslated sentence: Die Europäische Union ist eine einzigartige Partnerschaft zwischen 27 europäischen Ländern , die als Minister oder EU - Länder .\nAudio file saved as output.mp3. You can download it from the output files section.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# lower code run for training it \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:59:08.866417Z","iopub.execute_input":"2024-11-15T17:59:08.867278Z","iopub.status.idle":"2024-11-15T17:59:08.871271Z","shell.execute_reply.started":"2024-11-15T17:59:08.867234Z","shell.execute_reply":"2024-11-15T17:59:08.870321Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"warnings.filterwarnings('ignore') # Filtering warnings\nconfig = get_config() # Retrieving config settings\ntrain_model(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:58:55.427147Z","iopub.status.idle":"2024-11-15T17:58:55.427533Z","shell.execute_reply.started":"2024-11-15T17:58:55.427348Z","shell.execute_reply":"2024-11-15T17:58:55.427368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# warnings.filterwarnings('ignore') # Filtering warnings\n# config = get_config() # Retrieving config settings\n# # train_model(config, df)","metadata":{"id":"9pVM43RjsFUa","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:57:41.294153Z","iopub.status.idle":"2024-11-15T17:57:41.294619Z","shell.execute_reply.started":"2024-11-15T17:57:41.294372Z","shell.execute_reply":"2024-11-15T17:57:41.294397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"pUJRk073sFX3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"U8DwQqZssFeX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"D7NTkoP_sFhl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"2Eg2hXc0sFk_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"KKm_MCaasFoU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"rgkqFSjPsFsd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"HaRFmeoiolto","trusted":true},"outputs":[],"execution_count":null}]}